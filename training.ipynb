{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:22:18.697215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 20:22:19.762004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data.json\"\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 # percentage of dataset\n",
    "TEST_SPLIT = 0.1 # percentage of dataset\n",
    "\n",
    "NUM_SESSION_GROUPS = 5\n",
    "LOGDIR = \"logs/hparam_tuning/\"\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from json file\n",
    "\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "x = np.array(data[\"mfcc\"])\n",
    "y = np.array(data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation and test sets\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SPLIT)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "# add an axis to input sets to match the shape CNN expects (last axis is like channel in color images)\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "x_validation = x_validation[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose hyperparameters to tune\n",
    "\n",
    "# HP_CONV_LAYERS = hp.HParam(\"conv_layers\", hp.IntInterval(1, 3))\n",
    "# HP_CONV_KERNEL_SIZE = hp.HParam(\"conv_kernel_size\", hp.Discrete([3, 5]))\n",
    "# HP_POOL_SIZE = hp.HParam(\"conv_pool_size\", hp.Discrete([2, 3]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32, 64]))\n",
    "HP_DENSE_LAYERS = hp.HParam(\"dense_layers\", hp.IntInterval(1, 3))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.2, 0.3))\n",
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"sgd\"]))\n",
    "\n",
    "HPARAMS = [\n",
    "    # HP_CONV_LAYERS,\n",
    "    # HP_CONV_KERNEL_SIZE,\n",
    "    # HP_POOL_SIZE,\n",
    "    HP_NUM_UNITS,\n",
    "    HP_DENSE_LAYERS,\n",
    "    HP_DROPOUT,\n",
    "    HP_OPTIMIZER\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"accuracy (val)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"batch_accuracy\",\n",
    "        group=\"train\",\n",
    "        display_name=\"accuracy (train)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams, seed):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    INPUT_SHAPE = (x_train.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=INPUT_SHAPE))\n",
    "\n",
    "   #  conv_filters = 8\n",
    "   #  for _ in range(hparams[HP_CONV_LAYERS]):\n",
    "   #     model.add(\n",
    "   #        tf.keras.layers.Conv2D(\n",
    "   #           filters=conv_filters,\n",
    "   #          #  kernel_size=hparams[HP_CONV_KERNEL_SIZE],\n",
    "   #           kernel_size=3,\n",
    "   #           padding=\"same\",\n",
    "   #           activation=\"relu\"\n",
    "   #        )\n",
    "   #     )\n",
    "   #     model.add(\n",
    "   #        tf.keras.layers.MaxPooling2D(\n",
    "   #           pool_size=hparams[HP_POOL_SIZE],\n",
    "   #           strides=hparams[HP_POOL_SIZE]-1,\n",
    "   #           padding=\"same\"\n",
    "   #        )\n",
    "   #     )\n",
    "   #     model.add(tf.keras.layers.BatchNormalization())\n",
    "   #     conv_filters *= 2\n",
    "    \n",
    "    # 1st conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\\n\",\n",
    "    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    # flatten output and feed it into dense layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for _ in range(hparams[HP_DENSE_LAYERS]):\n",
    "        model.add(tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT], seed=rng.random()))\n",
    "\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "       optimizer=hparams[HP_OPTIMIZER],\n",
    "       loss='sparse_categorical_crossentropy',\n",
    "       metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(base_logdir, session_id, hparams):\n",
    "    model = create_model(hparams=hparams, seed=session_id)\n",
    "    logdir = os.path.join(base_logdir, session_id)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=logdir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "\n",
    "    hparams_callback = hp.KerasCallback(logdir, hparams)\n",
    "\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        callbacks=[tensorboard_callback, hparams_callback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(logdir, verbose=True):\n",
    "    rng = random.Random(0)\n",
    "\n",
    "    with tf.summary.create_file_writer(logdir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "\n",
    "    sessions_per_group = 2\n",
    "    num_sessions = NUM_SESSION_GROUPS * sessions_per_group\n",
    "    session_index = 0  # across all session groups\n",
    "    for group_index in range(NUM_SESSION_GROUPS):\n",
    "        hparams = {h: h.domain.sample_uniform(rng) for h in HPARAMS}\n",
    "        hparams_string = str(hparams)\n",
    "        for repeat_index in range(sessions_per_group):\n",
    "            session_id = str(session_index)\n",
    "            session_index += 1\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"--- Running training session %d/%d\"\n",
    "                    % (session_index, num_sessions)\n",
    "                )\n",
    "                print(hparams_string)\n",
    "                print(\"--- repeat #: %d\" % (repeat_index + 1))\n",
    "            run(\n",
    "                base_logdir=logdir,\n",
    "                session_id=session_id,\n",
    "                hparams=hparams,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to logs/hparam_tuning/\n",
      "--- Running training session 1/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.20404843781807777, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:23:16.067230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.214819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.215305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.216351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.216828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.217231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.422563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.423544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.423569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-19 20:23:16.424587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-19 20:23:16.424628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:23:18.405494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2023-12-19 20:23:18.708886: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-19 20:23:19.033554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9365bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-19 20:23:19.033633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 970, Compute Capability 5.2\n",
      "2023-12-19 20:23:19.238304: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-19 20:23:19.310873: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 6s 16ms/step - loss: 1.9546 - accuracy: 0.2960 - val_loss: 1.7150 - val_accuracy: 0.3883\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.4404 - accuracy: 0.4863 - val_loss: 1.5465 - val_accuracy: 0.4474\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.2199 - accuracy: 0.5669 - val_loss: 1.1954 - val_accuracy: 0.5875\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.0555 - accuracy: 0.6324 - val_loss: 1.4476 - val_accuracy: 0.5207\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.9420 - accuracy: 0.6701 - val_loss: 1.1251 - val_accuracy: 0.6093\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.8431 - accuracy: 0.7032 - val_loss: 0.9372 - val_accuracy: 0.6767\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.7514 - accuracy: 0.7428 - val_loss: 1.0332 - val_accuracy: 0.6418\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.6718 - accuracy: 0.7698 - val_loss: 1.0430 - val_accuracy: 0.6365\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5927 - accuracy: 0.7958 - val_loss: 0.9584 - val_accuracy: 0.6897\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5387 - accuracy: 0.8169 - val_loss: 0.7584 - val_accuracy: 0.7417\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4839 - accuracy: 0.8363 - val_loss: 0.7706 - val_accuracy: 0.7476\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4345 - accuracy: 0.8546 - val_loss: 0.8009 - val_accuracy: 0.7382\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3936 - accuracy: 0.8676 - val_loss: 0.8188 - val_accuracy: 0.7423\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3541 - accuracy: 0.8879 - val_loss: 0.7672 - val_accuracy: 0.7524\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3158 - accuracy: 0.8989 - val_loss: 1.1655 - val_accuracy: 0.6868\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2891 - accuracy: 0.9113 - val_loss: 0.8843 - val_accuracy: 0.7411\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2510 - accuracy: 0.9226 - val_loss: 0.7145 - val_accuracy: 0.7866\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2261 - accuracy: 0.9283 - val_loss: 0.7975 - val_accuracy: 0.7677\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2016 - accuracy: 0.9393 - val_loss: 0.9263 - val_accuracy: 0.7400\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1818 - accuracy: 0.9418 - val_loss: 0.8540 - val_accuracy: 0.7577\n",
      "--- Running training session 2/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.20404843781807777, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 2\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 4s 14ms/step - loss: 1.9632 - accuracy: 0.3137 - val_loss: 1.8315 - val_accuracy: 0.3611\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.4331 - accuracy: 0.4965 - val_loss: 1.3258 - val_accuracy: 0.5254\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.1822 - accuracy: 0.5903 - val_loss: 1.1377 - val_accuracy: 0.6022\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.9997 - accuracy: 0.6587 - val_loss: 0.9995 - val_accuracy: 0.6424\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.8627 - accuracy: 0.7085 - val_loss: 1.0958 - val_accuracy: 0.6194\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.7509 - accuracy: 0.7470 - val_loss: 0.8647 - val_accuracy: 0.6950\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.6503 - accuracy: 0.7791 - val_loss: 0.9791 - val_accuracy: 0.6743\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5847 - accuracy: 0.8061 - val_loss: 1.0925 - val_accuracy: 0.6596\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.5102 - accuracy: 0.8323 - val_loss: 0.8467 - val_accuracy: 0.7092\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4542 - accuracy: 0.8506 - val_loss: 0.9325 - val_accuracy: 0.7009\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4046 - accuracy: 0.8663 - val_loss: 0.8583 - val_accuracy: 0.7394\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3656 - accuracy: 0.8808 - val_loss: 0.7176 - val_accuracy: 0.7730\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3182 - accuracy: 0.8997 - val_loss: 1.0351 - val_accuracy: 0.6891\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2762 - accuracy: 0.9116 - val_loss: 0.8559 - val_accuracy: 0.7447\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2460 - accuracy: 0.9255 - val_loss: 0.9066 - val_accuracy: 0.7405\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 4s 18ms/step - loss: 0.2302 - accuracy: 0.9267 - val_loss: 0.9541 - val_accuracy: 0.7329\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 4s 17ms/step - loss: 0.1936 - accuracy: 0.9388 - val_loss: 0.9825 - val_accuracy: 0.7305\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 15ms/step - loss: 0.1780 - accuracy: 0.9455 - val_loss: 0.8551 - val_accuracy: 0.7713\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.1709 - accuracy: 0.9489 - val_loss: 0.9180 - val_accuracy: 0.7553\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.1385 - accuracy: 0.9586 - val_loss: 1.0035 - val_accuracy: 0.7388\n",
      "--- Running training session 3/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.29677999949201717, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 1\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 4s 14ms/step - loss: 1.8989 - accuracy: 0.3333 - val_loss: 1.6445 - val_accuracy: 0.4368\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.4157 - accuracy: 0.5148 - val_loss: 1.2135 - val_accuracy: 0.5674\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.2035 - accuracy: 0.5873 - val_loss: 1.1297 - val_accuracy: 0.6005\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.0418 - accuracy: 0.6460 - val_loss: 1.3685 - val_accuracy: 0.5331\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.9174 - accuracy: 0.6854 - val_loss: 1.0564 - val_accuracy: 0.6342\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.8124 - accuracy: 0.7315 - val_loss: 0.8775 - val_accuracy: 0.6974\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.7373 - accuracy: 0.7512 - val_loss: 0.8966 - val_accuracy: 0.6885\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.6554 - accuracy: 0.7834 - val_loss: 0.9411 - val_accuracy: 0.6785\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5965 - accuracy: 0.8041 - val_loss: 0.9388 - val_accuracy: 0.6927\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5401 - accuracy: 0.8261 - val_loss: 0.9640 - val_accuracy: 0.6956\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4792 - accuracy: 0.8443 - val_loss: 0.8133 - val_accuracy: 0.7459\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4304 - accuracy: 0.8604 - val_loss: 1.0083 - val_accuracy: 0.6868\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4037 - accuracy: 0.8681 - val_loss: 0.8329 - val_accuracy: 0.7275\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3687 - accuracy: 0.8831 - val_loss: 1.0237 - val_accuracy: 0.6939\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3344 - accuracy: 0.8913 - val_loss: 0.9460 - val_accuracy: 0.7270\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2967 - accuracy: 0.9078 - val_loss: 0.8138 - val_accuracy: 0.7506\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2665 - accuracy: 0.9174 - val_loss: 0.9812 - val_accuracy: 0.7199\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2518 - accuracy: 0.9192 - val_loss: 0.8665 - val_accuracy: 0.7541\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2198 - accuracy: 0.9298 - val_loss: 0.9619 - val_accuracy: 0.7382\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2025 - accuracy: 0.9360 - val_loss: 0.9130 - val_accuracy: 0.7500\n",
      "--- Running training session 4/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.29677999949201717, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 2\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 4s 16ms/step - loss: 1.9725 - accuracy: 0.3082 - val_loss: 1.7204 - val_accuracy: 0.4037\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.4651 - accuracy: 0.4874 - val_loss: 1.3809 - val_accuracy: 0.4935\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.2269 - accuracy: 0.5706 - val_loss: 1.1694 - val_accuracy: 0.5892\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.0828 - accuracy: 0.6353 - val_loss: 1.1264 - val_accuracy: 0.6212\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.9649 - accuracy: 0.6704 - val_loss: 1.0606 - val_accuracy: 0.6407\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.8421 - accuracy: 0.7166 - val_loss: 1.0529 - val_accuracy: 0.6543\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.7604 - accuracy: 0.7444 - val_loss: 1.0622 - val_accuracy: 0.6608\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.6845 - accuracy: 0.7685 - val_loss: 1.0900 - val_accuracy: 0.6566\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.6067 - accuracy: 0.7998 - val_loss: 1.0746 - val_accuracy: 0.6584\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.5489 - accuracy: 0.8206 - val_loss: 1.6110 - val_accuracy: 0.5786\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4879 - accuracy: 0.8353 - val_loss: 1.0053 - val_accuracy: 0.6874\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.4345 - accuracy: 0.8586 - val_loss: 0.9430 - val_accuracy: 0.7163\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3925 - accuracy: 0.8682 - val_loss: 0.9150 - val_accuracy: 0.7151\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3582 - accuracy: 0.8828 - val_loss: 1.0577 - val_accuracy: 0.7122\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.3228 - accuracy: 0.8936 - val_loss: 0.7990 - val_accuracy: 0.7553\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2888 - accuracy: 0.9048 - val_loss: 0.8980 - val_accuracy: 0.7411\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2582 - accuracy: 0.9152 - val_loss: 0.7520 - val_accuracy: 0.7819\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 0.2518 - accuracy: 0.9198 - val_loss: 0.8635 - val_accuracy: 0.7530\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2229 - accuracy: 0.9288 - val_loss: 0.9058 - val_accuracy: 0.7595\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2053 - accuracy: 0.9335 - val_loss: 0.9231 - val_accuracy: 0.7606\n",
      "--- Running training session 5/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 32, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.21392737051981528, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'adam'}\n",
      "--- repeat #: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:27:02.111645: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 6s 15ms/step - loss: 1.8840 - accuracy: 0.3220 - val_loss: 1.5723 - val_accuracy: 0.4616\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.2607 - accuracy: 0.5626 - val_loss: 1.1678 - val_accuracy: 0.5963\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.9577 - accuracy: 0.6755 - val_loss: 1.1538 - val_accuracy: 0.6395\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.7575 - accuracy: 0.7445 - val_loss: 1.1970 - val_accuracy: 0.6483\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.6250 - accuracy: 0.7930 - val_loss: 1.0871 - val_accuracy: 0.6797\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.5138 - accuracy: 0.8313 - val_loss: 1.1534 - val_accuracy: 0.6743\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4117 - accuracy: 0.8633 - val_loss: 1.0879 - val_accuracy: 0.6974\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3507 - accuracy: 0.8867 - val_loss: 1.1193 - val_accuracy: 0.6968\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3101 - accuracy: 0.8972 - val_loss: 1.0000 - val_accuracy: 0.7293\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2595 - accuracy: 0.9164 - val_loss: 1.2951 - val_accuracy: 0.6862\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.2503 - accuracy: 0.9187 - val_loss: 1.3262 - val_accuracy: 0.7015\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2078 - accuracy: 0.9301 - val_loss: 1.2794 - val_accuracy: 0.7222\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1788 - accuracy: 0.9415 - val_loss: 1.2508 - val_accuracy: 0.7370\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1726 - accuracy: 0.9477 - val_loss: 1.2869 - val_accuracy: 0.7423\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1769 - accuracy: 0.9458 - val_loss: 1.5856 - val_accuracy: 0.7045\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1471 - accuracy: 0.9520 - val_loss: 3.1228 - val_accuracy: 0.5875\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1403 - accuracy: 0.9527 - val_loss: 1.1854 - val_accuracy: 0.7630\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1250 - accuracy: 0.9592 - val_loss: 1.1961 - val_accuracy: 0.7624\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1151 - accuracy: 0.9619 - val_loss: 1.3971 - val_accuracy: 0.7429\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1330 - accuracy: 0.9597 - val_loss: 1.7568 - val_accuracy: 0.7057\n",
      "--- Running training session 6/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 32, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.21392737051981528, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'adam'}\n",
      "--- repeat #: 2\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 6s 15ms/step - loss: 1.8448 - accuracy: 0.3440 - val_loss: 1.6002 - val_accuracy: 0.4344\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 1.2626 - accuracy: 0.5610 - val_loss: 1.4085 - val_accuracy: 0.5278\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.9489 - accuracy: 0.6848 - val_loss: 1.5348 - val_accuracy: 0.5538\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.7453 - accuracy: 0.7494 - val_loss: 1.1387 - val_accuracy: 0.6383\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.6043 - accuracy: 0.7992 - val_loss: 1.5326 - val_accuracy: 0.5946\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.5071 - accuracy: 0.8308 - val_loss: 1.4958 - val_accuracy: 0.6241\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.4003 - accuracy: 0.8676 - val_loss: 1.0784 - val_accuracy: 0.7134\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3293 - accuracy: 0.8893 - val_loss: 0.9742 - val_accuracy: 0.7388\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3175 - accuracy: 0.8957 - val_loss: 1.3345 - val_accuracy: 0.6968\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2491 - accuracy: 0.9174 - val_loss: 1.0470 - val_accuracy: 0.7470\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2191 - accuracy: 0.9283 - val_loss: 1.5055 - val_accuracy: 0.6927\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2063 - accuracy: 0.9345 - val_loss: 1.2358 - val_accuracy: 0.7453\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1871 - accuracy: 0.9430 - val_loss: 1.0476 - val_accuracy: 0.7595\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1446 - accuracy: 0.9549 - val_loss: 1.1767 - val_accuracy: 0.7494\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1398 - accuracy: 0.9546 - val_loss: 1.1027 - val_accuracy: 0.7736\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1170 - accuracy: 0.9620 - val_loss: 1.8457 - val_accuracy: 0.7080\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1588 - accuracy: 0.9483 - val_loss: 1.4780 - val_accuracy: 0.7210\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1166 - accuracy: 0.9644 - val_loss: 1.1546 - val_accuracy: 0.7689\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1164 - accuracy: 0.9616 - val_loss: 1.3770 - val_accuracy: 0.7535\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.0973 - accuracy: 0.9688 - val_loss: 1.6621 - val_accuracy: 0.7323\n",
      "--- Running training session 7/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 32, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.2799402574081021, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'adam'}\n",
      "--- repeat #: 1\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 7s 16ms/step - loss: 1.9126 - accuracy: 0.3186 - val_loss: 1.6394 - val_accuracy: 0.4214\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 1.3649 - accuracy: 0.5201 - val_loss: 1.4630 - val_accuracy: 0.5225\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.0508 - accuracy: 0.6532 - val_loss: 1.1526 - val_accuracy: 0.6152\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.8307 - accuracy: 0.7246 - val_loss: 1.0793 - val_accuracy: 0.6448\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.6904 - accuracy: 0.7804 - val_loss: 1.3416 - val_accuracy: 0.5875\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.5650 - accuracy: 0.8203 - val_loss: 1.2404 - val_accuracy: 0.6596\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4653 - accuracy: 0.8483 - val_loss: 1.5467 - val_accuracy: 0.6324\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4082 - accuracy: 0.8686 - val_loss: 1.3716 - val_accuracy: 0.6661\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3459 - accuracy: 0.8941 - val_loss: 1.2646 - val_accuracy: 0.6956\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3171 - accuracy: 0.8975 - val_loss: 1.1800 - val_accuracy: 0.7181\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3024 - accuracy: 0.9028 - val_loss: 1.1780 - val_accuracy: 0.7074\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2533 - accuracy: 0.9176 - val_loss: 1.8624 - val_accuracy: 0.6377\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2170 - accuracy: 0.9294 - val_loss: 1.2646 - val_accuracy: 0.7199\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.2117 - accuracy: 0.9354 - val_loss: 1.2963 - val_accuracy: 0.7051\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1780 - accuracy: 0.9425 - val_loss: 1.6813 - val_accuracy: 0.7181\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1685 - accuracy: 0.9450 - val_loss: 1.3857 - val_accuracy: 0.7299\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1533 - accuracy: 0.9508 - val_loss: 1.2923 - val_accuracy: 0.7423\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1825 - accuracy: 0.9453 - val_loss: 1.7834 - val_accuracy: 0.6856\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1414 - accuracy: 0.9551 - val_loss: 1.6104 - val_accuracy: 0.7004\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1436 - accuracy: 0.9572 - val_loss: 1.7031 - val_accuracy: 0.7004\n",
      "--- Running training session 8/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 32, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.2799402574081021, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'adam'}\n",
      "--- repeat #: 2\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 6s 15ms/step - loss: 1.8325 - accuracy: 0.3409 - val_loss: 1.5680 - val_accuracy: 0.4297\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 1.3369 - accuracy: 0.5293 - val_loss: 1.3977 - val_accuracy: 0.5201\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.0373 - accuracy: 0.6485 - val_loss: 1.3175 - val_accuracy: 0.5786\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.8572 - accuracy: 0.7126 - val_loss: 1.1698 - val_accuracy: 0.6483\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.6984 - accuracy: 0.7733 - val_loss: 1.7452 - val_accuracy: 0.5626\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.5986 - accuracy: 0.8076 - val_loss: 1.6350 - val_accuracy: 0.6058\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4887 - accuracy: 0.8375 - val_loss: 1.2096 - val_accuracy: 0.6956\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.4343 - accuracy: 0.8577 - val_loss: 1.5000 - val_accuracy: 0.6401\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3647 - accuracy: 0.8818 - val_loss: 1.2677 - val_accuracy: 0.6897\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.3334 - accuracy: 0.8911 - val_loss: 1.2277 - val_accuracy: 0.6980\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.2786 - accuracy: 0.9121 - val_loss: 1.2768 - val_accuracy: 0.7080\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.2771 - accuracy: 0.9096 - val_loss: 1.4935 - val_accuracy: 0.6844\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.2412 - accuracy: 0.9181 - val_loss: 1.2169 - val_accuracy: 0.7382\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 3s 14ms/step - loss: 0.2218 - accuracy: 0.9272 - val_loss: 1.3785 - val_accuracy: 0.7252\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1980 - accuracy: 0.9347 - val_loss: 1.9723 - val_accuracy: 0.6365\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1907 - accuracy: 0.9379 - val_loss: 2.5620 - val_accuracy: 0.6093\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1628 - accuracy: 0.9486 - val_loss: 1.3265 - val_accuracy: 0.7394\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1637 - accuracy: 0.9483 - val_loss: 1.4180 - val_accuracy: 0.7110\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1536 - accuracy: 0.9517 - val_loss: 1.8875 - val_accuracy: 0.6590\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 0.1402 - accuracy: 0.9573 - val_loss: 1.8632 - val_accuracy: 0.6968\n",
      "--- Running training session 9/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.27298317482601286, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 1\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 4s 14ms/step - loss: 1.8472 - accuracy: 0.3638 - val_loss: 1.6608 - val_accuracy: 0.4037\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 3s 12ms/step - loss: 1.2922 - accuracy: 0.5449 - val_loss: 1.4163 - val_accuracy: 0.4970\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 3s 13ms/step - loss: 1.0691 - accuracy: 0.6257 - val_loss: 1.3773 - val_accuracy: 0.5313\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 4s 19ms/step - loss: 0.9530 - accuracy: 0.6739 - val_loss: 1.1765 - val_accuracy: 0.6046\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.8395 - accuracy: 0.7116 - val_loss: 1.0270 - val_accuracy: 0.6519\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.7491 - accuracy: 0.7454 - val_loss: 0.8729 - val_accuracy: 0.7009\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.6845 - accuracy: 0.7632 - val_loss: 0.8657 - val_accuracy: 0.7045\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 5s 22ms/step - loss: 0.6114 - accuracy: 0.7952 - val_loss: 0.8052 - val_accuracy: 0.7252\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 5s 22ms/step - loss: 0.5667 - accuracy: 0.8090 - val_loss: 0.8455 - val_accuracy: 0.7175\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.4993 - accuracy: 0.8338 - val_loss: 0.7397 - val_accuracy: 0.7559\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.4834 - accuracy: 0.8332 - val_loss: 0.8669 - val_accuracy: 0.7323\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.4385 - accuracy: 0.8525 - val_loss: 0.9052 - val_accuracy: 0.7104\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.3916 - accuracy: 0.8658 - val_loss: 0.8705 - val_accuracy: 0.7270\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.3743 - accuracy: 0.8762 - val_loss: 1.0196 - val_accuracy: 0.6950\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.3362 - accuracy: 0.8898 - val_loss: 0.8491 - val_accuracy: 0.7411\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.3305 - accuracy: 0.8870 - val_loss: 0.7978 - val_accuracy: 0.7595\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2910 - accuracy: 0.9043 - val_loss: 0.8301 - val_accuracy: 0.7482\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.2812 - accuracy: 0.9059 - val_loss: 0.6994 - val_accuracy: 0.7861\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2503 - accuracy: 0.9198 - val_loss: 0.8111 - val_accuracy: 0.7713\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2366 - accuracy: 0.9224 - val_loss: 0.6819 - val_accuracy: 0.8008\n",
      "--- Running training session 10/10\n",
      "{HParam(name='num_units', domain=Discrete([32, 64]), display_name=None, description=None): 64, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.2, 0.3), display_name=None, description=None): 0.27298317482601286, HParam(name='optimizer', domain=Discrete(['adam', 'sgd']), display_name=None, description=None): 'sgd'}\n",
      "--- repeat #: 2\n",
      "Epoch 1/20\n",
      "212/212 [==============================] - 6s 23ms/step - loss: 1.9195 - accuracy: 0.3580 - val_loss: 1.6435 - val_accuracy: 0.4297\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 5s 21ms/step - loss: 1.3174 - accuracy: 0.5375 - val_loss: 1.2793 - val_accuracy: 0.5626\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 1.0882 - accuracy: 0.6148 - val_loss: 1.5592 - val_accuracy: 0.5130\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.9213 - accuracy: 0.6798 - val_loss: 1.4446 - val_accuracy: 0.5561\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.8058 - accuracy: 0.7250 - val_loss: 1.2288 - val_accuracy: 0.6087\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.7261 - accuracy: 0.7566 - val_loss: 1.3074 - val_accuracy: 0.5916\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.6465 - accuracy: 0.7784 - val_loss: 1.0248 - val_accuracy: 0.6696\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.5782 - accuracy: 0.8057 - val_loss: 1.0479 - val_accuracy: 0.6720\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.5357 - accuracy: 0.8100 - val_loss: 1.0489 - val_accuracy: 0.6714\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.4891 - accuracy: 0.8387 - val_loss: 0.8135 - val_accuracy: 0.7305\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.4336 - accuracy: 0.8540 - val_loss: 0.9159 - val_accuracy: 0.7163\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 4s 20ms/step - loss: 0.4027 - accuracy: 0.8629 - val_loss: 0.7887 - val_accuracy: 0.7459\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.3699 - accuracy: 0.8787 - val_loss: 0.9974 - val_accuracy: 0.7033\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.3388 - accuracy: 0.8864 - val_loss: 0.7652 - val_accuracy: 0.7671\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.3199 - accuracy: 0.8957 - val_loss: 0.6799 - val_accuracy: 0.7831\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2913 - accuracy: 0.9028 - val_loss: 0.7608 - val_accuracy: 0.7813\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2723 - accuracy: 0.9077 - val_loss: 1.0185 - val_accuracy: 0.7080\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2542 - accuracy: 0.9165 - val_loss: 0.8452 - val_accuracy: 0.7553\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2318 - accuracy: 0.9235 - val_loss: 0.7908 - val_accuracy: 0.7707\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 4s 21ms/step - loss: 0.2165 - accuracy: 0.9292 - val_loss: 0.7993 - val_accuracy: 0.7553\n",
      "Done. Output saved to logs/hparam_tuning/\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "np.random.seed(0)\n",
    "logdir = LOGDIR\n",
    "shutil.rmtree(logdir, ignore_errors=True)\n",
    "print(f\"Saving output to {logdir}\")\n",
    "run_all(logdir=logdir, verbose=True)\n",
    "print(f\"Done. Output saved to {logdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-feede051a1713ec3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-feede051a1713ec3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
