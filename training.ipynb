{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data.json\"\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 # percentage of dataset\n",
    "TEST_SPLIT = 0.1 # percentage of dataset\n",
    "\n",
    "LOGDIR = \"logs/hparam_tuning/\"\n",
    "CHECKPOINT_DIR = \"logs/checkpoint\"\n",
    "\n",
    "NUM_SESSION_GROUPS = 1\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "COMPLEX_HPARAM_TUNING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from json file\n",
    "\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "x = np.array(data[\"mfcc\"])\n",
    "y = np.array(data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation and test sets\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SPLIT)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "# add an axis to input sets to match the shape CNN expects (last axis is like channel in color images)\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "x_validation = x_validation[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose hyperparameters to tune \n",
    "\n",
    "# some are blocked by an if condition due to machine memory constraints during tuning\n",
    "if COMPLEX_HPARAM_TUNING == True:\n",
    "    HP_CONV_LAYERS = hp.HParam(\"conv_layers\", hp.IntInterval(1, 3))\n",
    "    HP_CONV_KERNEL_SIZE = hp.HParam(\"conv_kernel_size\", hp.Discrete([3, 5]))\n",
    "    HP_POOL_SIZE = hp.HParam(\"conv_pool_size\", hp.Discrete([2, 3]))\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32, 64]))\n",
    "HP_DENSE_LAYERS = hp.HParam(\"dense_layers\", hp.IntInterval(1, 3))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.2, 0.3))\n",
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"sgd\"]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_NUM_UNITS,\n",
    "    HP_DENSE_LAYERS,\n",
    "    HP_DROPOUT,\n",
    "    HP_OPTIMIZER\n",
    "]\n",
    "\n",
    "if COMPLEX_HPARAM_TUNING == True:\n",
    "    HPARAMS.insert(0, HP_CONV_LAYERS)\n",
    "    HPARAMS.insert(1, HP_CONV_KERNEL_SIZE)\n",
    "    HPARAMS.insert(2, HP_POOL_SIZE)\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"accuracy (val)\",\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams, seed):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    INPUT_SHAPE = (x_train.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=INPUT_SHAPE))\n",
    "\n",
    "    if COMPLEX_HPARAM_TUNING == True:\n",
    "        conv_filters = 8\n",
    "        for _ in range(hparams[HP_CONV_LAYERS]):\n",
    "            model.add(\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=conv_filters,\n",
    "                    kernel_size=hparams[HP_CONV_KERNEL_SIZE],\n",
    "                    padding=\"same\",\n",
    "                    activation=\"relu\"\n",
    "                )\n",
    "            )\n",
    "            model.add(\n",
    "                tf.keras.layers.MaxPooling2D(\n",
    "                    pool_size=hparams[HP_POOL_SIZE],\n",
    "                    strides=hparams[HP_POOL_SIZE]-1,\n",
    "                    padding=\"same\"\n",
    "                )\n",
    "            )\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            conv_filters *= 2\n",
    "    else:\n",
    "        # 1st conv layer\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "        model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # 2nd conv layer\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # 3rd conv layer\\n\",\n",
    "        model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    # flatten output and feed it into dense layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for _ in range(hparams[HP_DENSE_LAYERS]):\n",
    "        model.add(tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT], seed=rng.random()))\n",
    "\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "       optimizer=hparams[HP_OPTIMIZER],\n",
    "       loss='sparse_categorical_crossentropy',\n",
    "       metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(base_logdir, session_id, hparams):\n",
    "    model = create_model(hparams=hparams, seed=session_id)\n",
    "    logdir = os.path.join(base_logdir, session_id)\n",
    "    checkpoint_dir = os.path.join(CHECKPOINT_DIR, session_id)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=logdir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "\n",
    "    hparams_callback = hp.KerasCallback(logdir, hparams)\n",
    "\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x=x_train[:BATCH_SIZE],\n",
    "        y=y_train[:BATCH_SIZE],\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        callbacks=[tensorboard_callback, hparams_callback, checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(logdir, verbose=True):\n",
    "    rng = random.Random(0)\n",
    "\n",
    "    with tf.summary.create_file_writer(logdir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "\n",
    "    # randomly select hyperparameters\n",
    "    sessions_per_group = 2\n",
    "    num_sessions = NUM_SESSION_GROUPS * sessions_per_group\n",
    "    session_index = 0  # across all session groups\n",
    "    for _ in range(NUM_SESSION_GROUPS):\n",
    "        hparams = {h: h.domain.sample_uniform(rng) for h in HPARAMS}\n",
    "        hparams_string = str(hparams)\n",
    "        for repeat_index in range(sessions_per_group):\n",
    "            session_id = str(session_index)\n",
    "            session_index += 1\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"--- Running training session %d/%d\"\n",
    "                    % (session_index, num_sessions)\n",
    "                )\n",
    "                print(hparams_string)\n",
    "                print(\"--- repeat #: %d\" % (repeat_index + 1))\n",
    "            model = run(\n",
    "                base_logdir=logdir,\n",
    "                session_id=session_id,\n",
    "                hparams=hparams,\n",
    "            )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "np.random.seed(0)\n",
    "logdir = LOGDIR\n",
    "shutil.rmtree(\"./logs/\", ignore_errors=True)\n",
    "print(f\"Saving output to {logdir}\")\n",
    "model = run_all(logdir=logdir, verbose=True)\n",
    "print(f\"Done. Output saved to {logdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from checkpoint\n",
    "\n",
    "# RUN_ID = \"0\"\n",
    "# model.load_weights(os.path.join(LOGDIR, RUN_ID))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
